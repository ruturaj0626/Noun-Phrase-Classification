{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17aeRSr-Phz0bx_DczACfArYOsD-84u97","timestamp":1693444017930}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:16:04.160126Z","start_time":"2018-12-30T16:15:59.777384Z"},"id":"huJYeymZGD_K","outputId":"b4c7c0b7-7670-4812-d87c-c39c4718713e","executionInfo":{"status":"ok","timestamp":1546188474235,"user_tz":-330,"elapsed":5262,"user":{"displayName":"Sumit Saha","photoUrl":"https://lh4.googleusercontent.com/-Fu7p2t_v6Rs/AAAAAAAAAAI/AAAAAAAAC6U/ts1cNKFt6bo/s64/photo.jpg","userId":"14951307279434050540"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"cell_type":"code","source":["!pip install contractions\n","!pip install nltk\n","!pip install gensim\n","\n","import numpy\n","from numpy.random import seed\n","import pandas as pd\n","import contractions\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","from gensim.models import FastText\n","\n","import string\n","import re\n","\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Flatten\n","from keras.layers.embeddings import Embedding\n","from keras import regularizers\n","from keras.models import model_from_json\n","from keras import backend\n","\n","from tensorflow import set_random_seed"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.17)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"metadata":{"id":"rvALEbfL_ato"},"cell_type":"markdown","source":["## Cleaning the data\n","\n","The `clean_data()` method reads the `file_name` passed onto it and does the following to each line of text\n","* Split the decision - `Yes` or `No` by `\\t` within a sentence and the lines by `\\n`\n","* Converts the contents to lower case\n","* Expands contractions, e.g. converts `you've` to `you have`\n","* Removes hyperlinks since they can be arbitrary which might throw our model off track\n","* Removes email addresses\n","* Removes numbers\n","* Removes punctuations\n","* Assigns integer value of `1` to `yes` and `0` to `no` for classification purposes\n","* Extracts noun-phrases from the file"]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:16:04.235408Z","start_time":"2018-12-30T16:16:04.162921Z"},"id":"sfUKwNttGT9b"},"cell_type":"code","source":["'''\n","clean_data() returns the feature set, decision\n","and the associated noun-phrases\"\n","'''\n","def clean_data(file):\n","    file = open(file, 'r', encoding=\"utf-8\")\n","    corpus = file.read()\n","\n","    # list of stopwords from the english language\n","    stop_words = stopwords.words('english')\n","    # retrieving punctuations from string module\n","    punctuations = [i for i in string.punctuation]\n","\n","    # splitting the data into (X,Y) for training/testing purposes by \"\\t\"\n","    # splitting the corpus into observations by \"\\n\"\n","    data = [i.split(\"\\t\") for i in corpus.split(\"\\n\")]\n","\n","    # decision vector - yes or no\n","    target = [t[0].lower() for t in data]\n","    # associated text\n","    text = [contractions.fix(t[1].lower()) for t in data]\n","\n","    # ---- Extracting Noun-Phrases from text ----\n","\n","    # \"noun_phrases\" contains individual lists of noun-phrases from all sentences\n","    noun_phrases = list()\n","    for sentence in text:\n","        # \"per_noun_phrases\" contain all noun-phrases from each sentence\n","        per_noun_phrases = list()\n","        # removing all integers from sentences\n","        sentence = re.sub(r'\\d+', \"\", sentence)\n","        # taking care of residual spaces after digit removal\n","        sentence = sentence.replace(\"  \", \" \")\n","        # tokenizing the sentence into words\n","        words = nltk.word_tokenize(sentence)\n","        # empty string \"np\" will take in individual noun-phrases\n","        np = ''\n","\n","        for w in words:\n","            if w not in stop_words+punctuations:\n","                np += w + ' '\n","            else:\n","                if np != '':\n","                    per_noun_phrases.append(np.strip())\n","                    np = ''\n","        noun_phrases.append(per_noun_phrases)\n","\n","    # remove hyperlinks\n","    text = [re.sub(r\"http\\S+\", \"\",t[1].lower()) for t in data]\n","    # remove email addresses\n","    text = [re.sub(r\"[^@]+@[^@]+\\.[^@]+\", \"\", t) for t in text]\n","\n","    # removing blank observations\n","    df = pd.DataFrame( {'text': text, 'target': target})\n","    df = df[df.iloc[:,0] != '']\n","    text = df['text'].values.tolist()\n","    target = df['target'].values.tolist()\n","\n","    # assigning unique indices to 'yes' and 'no'\n","    for x in target:\n","        if(x==\"no\"):\n","            target[target.index(x)] = 0\n","        else:\n","            target[target.index(x)] = 1\n","\n","    lemmatizer = WordNetLemmatizer()\n","\n","    for x in text:\n","        tokens = word_tokenize(x)\n","        tokens = [w.lower() for w in tokens]\n","        # removing punctuation\n","        table = str.maketrans('', '', string.punctuation)\n","        stripped = [w.translate(table) for w in tokens]\n","        # remove numbers\n","        words = [word for word in stripped if word.isalpha()]\n","        # filter out stop words\n","        stop_words = set(stopwords.words('english'))\n","        words = [lemmatizer.lemmatize(w) for w in words if not w in stop_words]\n","        text[text.index(x)] = words\n","\n","    return text, target, noun_phrases"],"execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:16:10.428644Z","start_time":"2018-12-30T16:16:04.238384Z"},"id":"oA36QP2bGuEI"},"cell_type":"code","source":["# retrieving cleaned data and noun-phrases\n","# from train and test datasets\n","text_train, target_train, noun_phrases_train = clean_data(\"train.txt\")\n","text_test, target_test, noun_phrases_test = clean_data(\"test.txt\")\n","\n","# all data tokenized\n","# converting back to cleaned sentences\n","train = [' '.join(x) for x in text_train]\n","test = [' '.join(x) for x in text_test]"],"execution_count":null,"outputs":[]},{"metadata":{"id":"OQjuExBN_atu"},"cell_type":"markdown","source":["## Generating Word Embeddings using FastText"]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:16:20.271876Z","start_time":"2018-12-30T16:16:10.431444Z"},"id":"NitOKBruHRv9"},"cell_type":"code","source":["# training word embeddings from a training corpus\n","# additional ability to obtain word vectors for out-of-vocabulary words.\n","model_ted = FastText(text_train + text_test, size=1000, window=5, min_count=5, workers=4,sg=1)"],"execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:16:20.396156Z","start_time":"2018-12-30T16:16:20.273610Z"},"id":"K_oOArXAIshu"},"cell_type":"code","source":["# 10,000 should be enough number of vocabulary items\n","vocab_size = 10000\n","# combines the train and test set\n","# finds one-hot vector of every word\n","encoded_docs = [one_hot(d, vocab_size) for d in train+test]\n","\n","# sequences are not of equal lengths\n","# keras requires all vectors to be of equal length\n","max_length = 131\n","padded_docs = pad_sequences(encoded_docs[:len(train)], maxlen=max_length)\n","padded_docs_test = pad_sequences(encoded_docs[len(train):], maxlen=max_length)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"iZYjJkbM_at1"},"cell_type":"markdown","source":["## Establishing the Neural Network"]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:40:24.350134Z","start_time":"2018-12-30T16:40:04.956181Z"},"id":"MNLOJx3FIw-6","outputId":"af669b32-71c1-469a-9ad8-fc69aaa654d0","executionInfo":{"status":"ok","timestamp":1546188579293,"user_tz":-330,"elapsed":8297,"user":{"displayName":"Sumit Saha","photoUrl":"https://lh4.googleusercontent.com/-Fu7p2t_v6Rs/AAAAAAAAAAI/AAAAAAAAC6U/ts1cNKFt6bo/s64/photo.jpg","userId":"14951307279434050540"}},"colab":{"base_uri":"https://localhost:8080/","height":797}},"cell_type":"code","source":["# to ensure reproducible results\n","seed(1)\n","if(len(backend.tensorflow_backend._get_available_gpus())==0):\n","    print(\"CPU runtime...\")\n","    set_random_seed(3)\n","else:\n","    print(\"GPU runtime...\")\n","    set_random_seed(2)\n","\n","# establishing a neural-network\n","# experimentation led to 18 epochs\n","# avoiding over-fitting and under-fitting\n","model = Sequential()\n","model.add(Embedding(vocab_size, 8, input_length=max_length))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","\n","'''\n","AdaDelta no manual tuning of a learning rate and\n","appears robust to noisy gradient information,\n","different model architecture choices, various data modalities\n","and selection of hyperparameters.\n","'''\n","model.compile(optimizer=\"adadelta\", loss='mean_squared_error', metrics=['acc'])\n","\n","model.fit(padded_docs, target_train, epochs=20, verbose=1,batch_size=128)\n","\n","# testing for accuracy on test dataset\n","print()\n","loss, accuracy = model.evaluate(padded_docs_test, target_test, verbose=1)\n","print(\"Accuracy: {0:.2f} %\".format(accuracy*100))\n","\n","# saving the model for future use\n","try:\n","    save = input(\"\\nSave Model? (yes=1, no=otherwise): \")\n","    if(save=='1'):\n","        name = input(\"Save as: \")\n","        model_json = model.to_json()\n","        with open(\"{}.json\".format(name), \"w\") as json_file:\n","            json_file.write(model_json)\n","        # serialize weights to HDF5\n","        model.save_weights(\"{}.h5\".format(name))\n","        print(\"Saved {0}.json and {0}.h5 to disk\".format(name))\n","    else:\n","        print(\"Model not saved\")\n","except:\n","    pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU runtime...\n","Epoch 1/20\n","3657/3657 [==============================] - 0s 105us/step - loss: 0.2494 - acc: 0.5264\n","Epoch 2/20\n","3657/3657 [==============================] - 0s 35us/step - loss: 0.2487 - acc: 0.5299\n","Epoch 3/20\n","3657/3657 [==============================] - 0s 35us/step - loss: 0.2479 - acc: 0.5313\n","Epoch 4/20\n","3657/3657 [==============================] - 0s 36us/step - loss: 0.2471 - acc: 0.5327\n","Epoch 5/20\n","3657/3657 [==============================] - 0s 38us/step - loss: 0.2457 - acc: 0.5543\n","Epoch 6/20\n","3657/3657 [==============================] - 0s 37us/step - loss: 0.2435 - acc: 0.5740\n","Epoch 7/20\n","3657/3657 [==============================] - 0s 39us/step - loss: 0.2416 - acc: 0.5721\n","Epoch 8/20\n","3657/3657 [==============================] - 0s 35us/step - loss: 0.2379 - acc: 0.6073\n","Epoch 9/20\n","3657/3657 [==============================] - 0s 36us/step - loss: 0.2339 - acc: 0.6284\n","Epoch 10/20\n","3657/3657 [==============================] - 0s 35us/step - loss: 0.2279 - acc: 0.6735\n","Epoch 11/20\n","3657/3657 [==============================] - 0s 35us/step - loss: 0.2207 - acc: 0.6842\n","Epoch 12/20\n","3657/3657 [==============================] - 0s 35us/step - loss: 0.2134 - acc: 0.7077\n","Epoch 13/20\n","3657/3657 [==============================] - 0s 35us/step - loss: 0.2050 - acc: 0.7301\n","Epoch 14/20\n","3657/3657 [==============================] - 0s 36us/step - loss: 0.1972 - acc: 0.7405\n","Epoch 15/20\n","3657/3657 [==============================] - 0s 39us/step - loss: 0.1902 - acc: 0.7441\n","Epoch 16/20\n","3657/3657 [==============================] - 0s 36us/step - loss: 0.1841 - acc: 0.7531\n","Epoch 17/20\n","3657/3657 [==============================] - 0s 36us/step - loss: 0.1788 - acc: 0.7566\n","Epoch 18/20\n","3657/3657 [==============================] - 0s 37us/step - loss: 0.1734 - acc: 0.7700\n","Epoch 19/20\n","3657/3657 [==============================] - 0s 35us/step - loss: 0.1694 - acc: 0.7670\n","Epoch 20/20\n","3657/3657 [==============================] - 0s 36us/step - loss: 0.1651 - acc: 0.7739\n","\n","992/992 [==============================] - 0s 149us/step\n","Accuracy: 77.72 %\n"],"name":"stdout"}]},{"metadata":{"id":"eBHW0buo_at5"},"cell_type":"markdown","source":["## Loading the model to run to test dataset"]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:32:34.121667Z","start_time":"2018-12-30T16:32:31.621314Z"},"id":"xtznQQCG_at6","outputId":"536fe373-234e-49cf-bf18-454d08f571fe","executionInfo":{"status":"ok","timestamp":1546189558308,"user_tz":-330,"elapsed":12673,"user":{"displayName":"Sumit Saha","photoUrl":"https://lh4.googleusercontent.com/-Fu7p2t_v6Rs/AAAAAAAAAAI/AAAAAAAAC6U/ts1cNKFt6bo/s64/photo.jpg","userId":"14951307279434050540"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["models = !ls \"\"*.{h5,json}\n","models = models[0].split(\"\\t\")\n","\n","mo = list()\n","for m in models:\n","    mod = m.rsplit('.', 1)[0]\n","    if mod not in mo:\n","        mo.append(mod)\n","models = mo\n","try:\n","    name = input(\"Load model: {}\\n\".format(models))\n","    if name not in models:\n","        print(\"Model: {} not available. \\nLoading model: model\".format(name))\n","        name = \"model\"\n","except:\n","    print(\"Loading model: model\")\n","    name = \"model\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load model: ['model_GPU_acc_80.54']\n","model_GPU_acc_80.54\n"],"name":"stdout"}]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:33:32.680116Z","start_time":"2018-12-30T16:33:32.480880Z"},"id":"mwJg9XeyKSj2","outputId":"5fa8cdc9-47fb-4220-a1d2-9f1f93d4734c","executionInfo":{"status":"ok","timestamp":1546189561004,"user_tz":-330,"elapsed":1272,"user":{"displayName":"Sumit Saha","photoUrl":"https://lh4.googleusercontent.com/-Fu7p2t_v6Rs/AAAAAAAAAAI/AAAAAAAAC6U/ts1cNKFt6bo/s64/photo.jpg","userId":"14951307279434050540"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["json_file = open('{}.json'.format(name), 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","print(\"Loaded {}.json from disk\".format(name))\n","# load weights into new model\n","loaded_model.load_weights(\"{}.h5\".format(name))\n","print(\"Loaded {}.h5 from disk\".format(name))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loaded model_GPU_acc_80.54.json from disk\n","Loaded model_GPU_acc_80.54.h5 from disk\n"],"name":"stdout"}]},{"metadata":{"ExecuteTime":{"end_time":"2018-12-30T16:34:18.917199Z","start_time":"2018-12-30T16:34:18.661777Z"},"id":"iaEXs_6zKU2s","outputId":"bd1728c2-3f4f-4cfe-e9c2-0c50b88f9813","executionInfo":{"status":"ok","timestamp":1546189563073,"user_tz":-330,"elapsed":1013,"user":{"displayName":"Sumit Saha","photoUrl":"https://lh4.googleusercontent.com/-Fu7p2t_v6Rs/AAAAAAAAAAI/AAAAAAAAC6U/ts1cNKFt6bo/s64/photo.jpg","userId":"14951307279434050540"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","score = loaded_model.evaluate(padded_docs_test, target_test, verbose=1)\n","print(\"{0} Accuracy: {1:.2f} %\".format(name ,(score[1]*100)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["992/992 [==============================] - 0s 219us/step\n","model_GPU_acc_80.54 Accuracy: 80.54 %\n"],"name":"stdout"}]},{"metadata":{"id":"2Pec4UjQKA5K"},"cell_type":"markdown","source":["## Noun-Phrase Detection Example"]},{"metadata":{"id":"cFUfh4k6J_10","outputId":"1f1fbbad-d23b-4716-8a8c-e01e55a1a091","executionInfo":{"status":"ok","timestamp":1546190868761,"user_tz":-330,"elapsed":1187,"user":{"displayName":"Sumit Saha","photoUrl":"https://lh4.googleusercontent.com/-Fu7p2t_v6Rs/AAAAAAAAAAI/AAAAAAAAC6U/ts1cNKFt6bo/s64/photo.jpg","userId":"14951307279434050540"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["text = ['Today is a very great day. Indian politicians are very corrupt']\n","noun_phrases_eg = list()\n","# list of stopwords from the english language\n","stop_words = stopwords.words('english')\n","# retrieving punctuations from string module\n","punctuations = [i for i in string.punctuation]\n","for sentence in text:\n","    # \"per_noun_phrases\" contain all noun-phrases from each sentence\n","    per_noun_phrases = list()\n","    # removing all integers from sentences\n","    sentence = re.sub(r'\\d+', \"\", sentence)\n","    # taking care of residual spaces after digit removal\n","    sentence = sentence.replace(\"  \", \" \")\n","    # tokenizing the sentence into words\n","    words = nltk.word_tokenize(sentence)\n","    # empty string \"np\" will take in individual noun-phrases\n","    np = ''\n","\n","    for w in words:\n","        if w not in stop_words+punctuations:\n","            np += w + ' '\n","        else:\n","            if np != '':\n","                per_noun_phrases.append(np.strip())\n","                np = ''\n","    noun_phrases_eg.append(per_noun_phrases)\n","print(noun_phrases_eg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['Today', 'great day', 'Indian politicians']]\n"],"name":"stdout"}]},{"metadata":{"id":"oIcunJ7hKEwA"},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}]}